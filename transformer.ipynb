{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer implementation from https://wingedsheep.com/building-a-language-model/ (and a little bit from x-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class TokenEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch module that converts tokens into embeddings.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length)\n",
    "    Output dimension is: (batch_size, sequence_length, d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, number_of_tokens):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(\n",
    "            num_embeddings=number_of_tokens,\n",
    "            embedding_dim=d_model\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding_layer(x)\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module that creates a positional encoding matrix. This matrix will later be added to the\n",
    "    transformer's input embeddings to provide a sense of position of the sequence elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.positional_encoding = self.create_positional_encoding()\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        \"\"\"\n",
    "        Creates a positional encoding matrix of size (max_sequence_length, d_model).\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize positional encoding matrix\n",
    "        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
    "\n",
    "        # Calculate positional encoding for each position and each dimension\n",
    "        for pos in range(self.max_sequence_length):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                # Apply sin to even indices in the array; indices in Python start at 0 so i is even.\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "                if i + 1 < self.d_model:\n",
    "                    # Apply cos to odd indices in the array; we add 1 to i because indices in Python start at 0.\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "        # Convert numpy array to PyTorch tensor and return it\n",
    "        return torch.from_numpy(positional_encoding).float().to(get_device())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds the positional encoding to the input embeddings at the corresponding positions.\n",
    "        \"\"\"\n",
    "        # Add positional encodings to input embeddings. The \":\" indexing ensures we only add positional encodings up\n",
    "        # to the length of the sequence in the batch. x.size(0) is the batch size, so this is a way to make sure\n",
    "        # we're not adding extra positional encodings.\n",
    "        return x + self.positional_encoding[:x.size(1), :]\n",
    "\n",
    "\n",
    "class MaskedSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a self attention layer.\n",
    "    This layer is used in the MultiHeadedSelfAttention module.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, head_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, head_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = head_dimension\n",
    "        self.query_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.key_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.value_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the self attention.\n",
    "\n",
    "        x dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "        output dimension is: (batch_size, sequence_length, head_dimension)\n",
    "        mask dimension is: (batch_size, sequence_length)\n",
    "\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "\n",
    "        # x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        # query, key, value dimensions are: (batch_size, sequence_length, head_dimension)\n",
    "        query = self.query_layer(x)\n",
    "        key = self.key_layer(x)\n",
    "        value = self.value_layer(x)\n",
    "\n",
    "        # Calculate the attention weights.\n",
    "        # attention_weights dimensions are: (batch_size, sequence_length, sequence_length)\n",
    "        attention_weights = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "        # Scale the attention weights.\n",
    "        attention_weights = attention_weights / np.sqrt(self.head_dimension)\n",
    "\n",
    "        # Apply the mask to the attention weights, by setting the masked tokens to a very low value.\n",
    "        # This will make the softmax output 0 for these values.\n",
    "        mask = mask.reshape(attention_weights.shape[0], 1, attention_weights.shape[2])\n",
    "        attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Softmax makes sure all scores are between 0 and 1 and the sum of scores is 1.\n",
    "        # attention_scores dimensions are: (batch_size, sequence_length, sequence_length)\n",
    "        attention_scores = self.softmax(attention_weights)\n",
    "\n",
    "        # The attention scores are multiplied by the value\n",
    "        # Values of tokens with high attention score get highlighted because they are multiplied by a larger number,\n",
    "        # and tokens with low attention score get drowned out because they are multiplied by a smaller number.\n",
    "        # Output dimensions are: (batch_size, sequence_length, head_dimension)\n",
    "        return torch.bmm(attention_scores, value)\n",
    "\n",
    "\n",
    "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a multi head attention layer.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, number_of_heads):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = embedding_dimension // number_of_heads\n",
    "        self.number_of_heads = number_of_heads\n",
    "\n",
    "        # Create the self attention modules\n",
    "        self.self_attentions = torch.nn.ModuleList(\n",
    "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(number_of_heads)])\n",
    "\n",
    "        # Create a linear layer to combine the outputs of the self attention modules\n",
    "        self.output_layer = torch.nn.Linear(number_of_heads * self.head_dimension, embedding_dimension)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the multi head attention.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        mask dimensions are: (batch_size, sequence_length)\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "        # Compute the self attention for each head\n",
    "        # self_attention_outputs dimensions are:\n",
    "        # (number_of_heads, batch_size, sequence_length, head_dimension)\n",
    "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
    "\n",
    "        # Concatenate the self attention outputs\n",
    "        # self_attention_outputs_concatenated dimensions are:\n",
    "        # (batch_size, sequence_length, number_of_heads * head_dimension)\n",
    "        concatenated_self_attention_outputs = torch.cat(self_attention_outputs, dim=2)\n",
    "\n",
    "        # Apply the output layer to the concatenated self attention outputs\n",
    "        # output dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        return self.output_layer(concatenated_self_attention_outputs)\n",
    "\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a feed forward layer.\n",
    "\n",
    "    A feed forward layer is a fully connected layer with a ReLU activation function in between.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, feed_forward_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.linear_1 = torch.nn.Linear(embedding_dimension, feed_forward_dimension)\n",
    "        self.linear_2 = torch.nn.Linear(feed_forward_dimension, embedding_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the feed forward layer.\n",
    "        \"\"\"\n",
    "        return self.linear_2(torch.relu(self.linear_1(x)))\n",
    "\n",
    "\n",
    "class DecoderLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for an encoder layer.\n",
    "\n",
    "    An encoder layer consists of a multi-headed self attention layer, a feed forward layer and dropout.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dimension,\n",
    "            number_of_heads,\n",
    "            feed_forward_dimension,\n",
    "            dropout_rate\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.multi_headed_self_attention = MaskedMultiHeadedSelfAttention(embedding_dimension, number_of_heads)\n",
    "        self.feed_forward = FeedForward(embedding_dimension, feed_forward_dimension)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.layer_normalization_1 = torch.nn.LayerNorm(embedding_dimension)\n",
    "        self.layer_normalization_2 = torch.nn.LayerNorm(embedding_dimension)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the encoder layer.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        mask dimensions are: (batch_size, sequence_length)\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layer normalization 1\n",
    "        normalized_x = self.layer_normalization_1(x)\n",
    "\n",
    "        # Multi headed self attention\n",
    "        attention_output = self.multi_headed_self_attention(normalized_x, mask)\n",
    "\n",
    "        # Residual output\n",
    "        residual_output = x + attention_output\n",
    "\n",
    "        # Layer normalization 2\n",
    "        normalized_residual_output = self.layer_normalization_2(residual_output)\n",
    "\n",
    "        # Feed forward\n",
    "        feed_forward_output = self.feed_forward(normalized_residual_output)\n",
    "\n",
    "        # Dropout\n",
    "        if self.training:\n",
    "            feed_forward_output = self.dropout(feed_forward_output)\n",
    "\n",
    "        # Residual output\n",
    "        return residual_output + feed_forward_output\n",
    "\n",
    "\n",
    "class DecoderStack(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The decoder stack consists of multiple decoder layers in sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dimension,\n",
    "            number_of_layers,\n",
    "            number_of_heads,\n",
    "            feed_forward_dimension,\n",
    "            dropout_rate,\n",
    "            max_sequence_length\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "        # Create the encoder layers\n",
    "        self.encoder_layers = torch.nn.ModuleList(\n",
    "            [DecoderLayer(embedding_dimension, number_of_heads, feed_forward_dimension, dropout_rate) for _ in\n",
    "             range(number_of_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        decoder_outputs = x\n",
    "        for decoder_layer in self.encoder_layers:\n",
    "            decoder_outputs = decoder_layer(decoder_outputs, mask)\n",
    "\n",
    "        return decoder_outputs\n",
    "\n",
    "\n",
    "class LMHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for the language model head.\n",
    "    The language model head is a linear layer that maps the embedding dimension to the vocabulary size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, number_of_tokens):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_tokens = number_of_tokens\n",
    "        self.linear = torch.nn.Linear(embedding_dimension, number_of_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the language model head.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
    "        \"\"\"\n",
    "        # Compute the linear layer\n",
    "        # linear_output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
    "        linear_output = self.linear(x)\n",
    "\n",
    "        return linear_output\n",
    "\n",
    "\n",
    "class LanguageModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            number_of_tokens,  # The number of tokens in the vocabulary\n",
    "            max_sequence_length=512,  # The maximum sequence length to use for attention\n",
    "            embedding_dimension=512,  # The dimension of the token embeddings\n",
    "            number_of_layers=6,  # The number of decoder layers to use\n",
    "            number_of_heads=4,  # The number of attention heads to use\n",
    "            feed_forward_dimension=None,  # The dimension of the feed forward layer\n",
    "            dropout_rate=0.1  # The dropout rate to use\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.number_of_tokens = number_of_tokens\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.number_of_heads = number_of_heads\n",
    "\n",
    "        if feed_forward_dimension is None:\n",
    "            # GPT-2 paper uses 4 * embedding_dimension for the feed forward dimension\n",
    "            self.feed_forward_dimension = embedding_dimension * 4\n",
    "        else:\n",
    "            self.feed_forward_dimension = feed_forward_dimension\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Create the token embedding layer\n",
    "        self.token_embedding = TokenEmbedding(embedding_dimension, number_of_tokens)\n",
    "\n",
    "        # Create the positional encoding layer\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dimension, max_sequence_length)\n",
    "\n",
    "        # Create the normalization layer\n",
    "        self.layer_normalization = torch.nn.LayerNorm(embedding_dimension)\n",
    "\n",
    "        # Create the decoder stack\n",
    "        self.decoder = DecoderStack(\n",
    "            embedding_dimension=embedding_dimension,\n",
    "            number_of_layers=number_of_layers,\n",
    "            number_of_heads=number_of_heads,\n",
    "            feed_forward_dimension=self.feed_forward_dimension,\n",
    "            dropout_rate=dropout_rate,\n",
    "            max_sequence_length=max_sequence_length\n",
    "        )\n",
    "\n",
    "        # Create the language model head\n",
    "        self.lm_head = LMHead(embedding_dimension, number_of_tokens)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Compute the token embeddings\n",
    "        # token_embeddings dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        token_embeddings = self.token_embedding(x)\n",
    "\n",
    "        # Compute the positional encoding\n",
    "        # positional_encoding dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        positional_encoding = self.positional_encoding(token_embeddings)\n",
    "\n",
    "        # Post embedding layer normalization\n",
    "        positional_encoding_normalized = self.layer_normalization(positional_encoding)\n",
    "\n",
    "        decoder_outputs = self.decoder(positional_encoding_normalized, mask)\n",
    "        lm_head_outputs = self.lm_head(decoder_outputs)\n",
    "\n",
    "        return lm_head_outputs\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        print(f'Saving checkpoint {path}')\n",
    "        torch.save({\n",
    "            'number_of_tokens': self.number_of_tokens,\n",
    "            'max_sequence_length': self.max_sequence_length,\n",
    "            'embedding_dimension': self.embedding_dimension,\n",
    "            'number_of_layers': self.number_of_layers,\n",
    "            'number_of_heads': self.number_of_heads,\n",
    "            'feed_forward_dimension': self.feed_forward_dimension,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'model_state_dict': self.state_dict()\n",
    "        }, path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(path) -> 'LanguageModel':\n",
    "        checkpoint = torch.load(path)\n",
    "        model = LanguageModel(\n",
    "            number_of_tokens=checkpoint['number_of_tokens'],\n",
    "            max_sequence_length=checkpoint['max_sequence_length'],\n",
    "            embedding_dimension=checkpoint['embedding_dimension'],\n",
    "            number_of_layers=checkpoint['number_of_layers'],\n",
    "            number_of_heads=checkpoint['number_of_heads'],\n",
    "            feed_forward_dimension=checkpoint['feed_forward_dimension'],\n",
    "            dropout_rate=checkpoint['dropout_rate']\n",
    "        )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return model.to(get_device())\n",
    "\n",
    "\n",
    "class AutoregressiveWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module that wraps a GPT model and makes it autoregressive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gpt_model):\n",
    "        super().__init__()\n",
    "        self.model = gpt_model\n",
    "        self.max_sequence_length = self.model.max_sequence_length\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Autoregressive forward pass\n",
    "        \"\"\"\n",
    "        inp, target = x[:, :-1], x[:, 1:]\n",
    "        mask = mask[:, :-1]\n",
    "\n",
    "        output = self.model(inp, mask)\n",
    "        return output, target\n",
    "\n",
    "    def next_token_probabilities(self, x, mask, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Calculate the token probabilities for the next token in the sequence.\n",
    "        \"\"\"\n",
    "        logits = self.model(x, mask)[:, -1]\n",
    "\n",
    "        # Apply temperature\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        # Apply the softmax\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        self.model.save_checkpoint(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(path) -> 'AutoregressiveWrapper':\n",
    "        model = LanguageModel.load_checkpoint(path)\n",
    "        return AutoregressiveWrapper(model).to(get_device())\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = {}\n",
    "\n",
    "        # Add the padding token\n",
    "        self.__add_to_dict('<pad>')\n",
    "\n",
    "        # Add characters and numbers to the dictionary\n",
    "        for i in range(10):\n",
    "            self.__add_to_dict(str(i))\n",
    "        for i in range(26):\n",
    "            self.__add_to_dict(chr(ord('a') + i))\n",
    "\n",
    "        # Add space and punctuation to the dictionary\n",
    "        self.__add_to_dict('.')\n",
    "        self.__add_to_dict(' ')\n",
    "\n",
    "    def __add_to_dict(self, character):\n",
    "        if character not in self.dictionary:\n",
    "            self.dictionary[character] = len(self.dictionary)\n",
    "            self.reverse_dictionary[self.dictionary[character]] = character\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.dictionary[c] for c in text]\n",
    "\n",
    "    def character_to_token(self, character):\n",
    "        return self.dictionary[character]\n",
    "\n",
    "    def token_to_character(self, token):\n",
    "        return self.reverse_dictionary[token]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.dictionary)\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, tokenizer: Tokenizer, optimizer=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, data: List[str], epochs, batch_size):\n",
    "        loss_per_epoch = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "\n",
    "            # Shuffle the sequences\n",
    "            random.shuffle(data)\n",
    "\n",
    "            # Create batches of sequences and their respective mask.\n",
    "            batches = []\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                sequence_tensor = torch.tensor(data[i: i + batch_size], dtype=torch.long)\n",
    "\n",
    "                # Create the mask tensor for the batch, where 1 means the token is not a padding token\n",
    "                mask_tensor = torch.ones_like(sequence_tensor)\n",
    "                mask_tensor[sequence_tensor == self.tokenizer.character_to_token('<pad>')] = 0\n",
    "\n",
    "                batches.append((sequence_tensor, mask_tensor))\n",
    "\n",
    "            # Train the model on each batch\n",
    "            for batch in batches:\n",
    "                self.model.train()\n",
    "\n",
    "                # Create the input and mask tensors\n",
    "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
    "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
    "\n",
    "                for i, input_entry in enumerate(batch[0]):\n",
    "                    input_tensor[i] = input_entry\n",
    "\n",
    "                for i, mask_entry in enumerate(batch[1]):\n",
    "                    mask_tensor[i] = mask_entry\n",
    "\n",
    "                # Compute the model output\n",
    "                model_output, target = self.model.forward(\n",
    "                    x=input_tensor.to(get_device()),\n",
    "                    mask=mask_tensor.to(get_device())\n",
    "                )\n",
    "\n",
    "                # Compute the losses\n",
    "                # The loss is computed on the model output and the target\n",
    "                loss = self.loss_function(model_output.transpose(1, 2), target)\n",
    "                # loss = self.loss_function(model_output[:, -1, :], target[:, -1])\n",
    "\n",
    "                # Backpropagate the loss.\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the gradients. This is used to prevent exploding gradients.\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "\n",
    "                # Update the model parameters. This is done by taking a step in the direction of the gradient.\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Reset the gradients. This is done so that the gradients from the previous batch\n",
    "                # are not used in the next step.\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Append the loss to the list of losses, so that the average loss can be computed for this epoch.\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Print the loss\n",
    "            epoch_loss = np.average(losses)\n",
    "            loss_per_epoch.append(epoch_loss)\n",
    "            print('Epoch:', epoch, 'Loss:', epoch_loss)\n",
    "\n",
    "        return loss_per_epoch\n",
    "\n",
    "\n",
    "class Generator:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate(\n",
    "            self,\n",
    "            max_tokens_to_generate: int,\n",
    "            prompt: str = None,\n",
    "            temperature: float = 1.0,\n",
    "            eos_token: int = None,\n",
    "            padding_token: int = 0):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        if prompt is None:\n",
    "            start_tokens = [self.tokenizer.character_to_token(padding_token)]\n",
    "        else:\n",
    "            start_tokens = self.tokenizer.tokenize(prompt)\n",
    "\n",
    "        input_tensor = torch.tensor(\n",
    "            pad_left(\n",
    "                sequence=start_tokens,\n",
    "                final_length=self.model.max_sequence_length + 1,\n",
    "                padding_token=padding_token\n",
    "            ),\n",
    "            dtype=torch.long\n",
    "        ).to(get_device())\n",
    "\n",
    "        num_dims = len(input_tensor.shape)\n",
    "\n",
    "        if num_dims == 1:\n",
    "            input_tensor = input_tensor[None, :]\n",
    "\n",
    "        out = input_tensor\n",
    "        for _ in range(max_tokens_to_generate):\n",
    "\n",
    "            x = out[:, -self.model.max_sequence_length:]\n",
    "\n",
    "            mask = torch.ones_like(x)\n",
    "            mask[x == padding_token] = 0\n",
    "\n",
    "            # Compute the next token probabilities\n",
    "            next_token_probabilities = self.model.next_token_probabilities(\n",
    "                x=x,\n",
    "                temperature=temperature,\n",
    "                mask=mask\n",
    "            )\n",
    "\n",
    "            # Sample the next token from the probability distribution\n",
    "            next_token = torch.multinomial(next_token_probabilities, num_samples=1)\n",
    "\n",
    "            # Append the next token to the output\n",
    "            out = torch.cat([out, next_token], dim=1)\n",
    "\n",
    "            # If the end of sequence token is reached, stop generating tokens\n",
    "            if eos_token is not None and next_token == eos_token:\n",
    "                break\n",
    "\n",
    "        generated_tokens = out[0].tolist()\n",
    "        return ''.join([self.tokenizer.token_to_character(token) for token in generated_tokens])\n",
    "\n",
    "\n",
    "def create_training_sequences(max_sequence_length, tokenized_training_data):\n",
    "    # Create sequences of length max_sequence_length + 1\n",
    "    # The last token of each sequence is the target token\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n",
    "        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\n",
    "    # Tokenize the training data\n",
    "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
    "    for _ in range(max_sequence_length):\n",
    "        # Prepend padding tokens\n",
    "        tokenized_training_data.insert(0, tokenizer.character_to_token('<pad>'))\n",
    "    return tokenized_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2.644440096616745\n",
      "Epoch: 1 Loss: 1.919064012169838\n",
      "Epoch: 2 Loss: 1.6657718479633332\n",
      "Epoch: 3 Loss: 1.489954298734665\n",
      "Epoch: 4 Loss: 1.3466267257928848\n",
      "Epoch: 5 Loss: 1.2159163817763328\n",
      "Epoch: 6 Loss: 1.0945930540561677\n",
      "Epoch: 7 Loss: 0.9826049834489823\n",
      "Epoch: 8 Loss: 0.8793779790401459\n",
      "Epoch: 9 Loss: 0.7762208022177219\n",
      "Epoch: 10 Loss: 0.70168916285038\n",
      "Epoch: 11 Loss: 0.6262750118970871\n",
      "Epoch: 12 Loss: 0.5616555750370026\n",
      "Epoch: 13 Loss: 0.49074860736727716\n",
      "Epoch: 14 Loss: 0.42440066337585447\n",
      "Epoch: 15 Loss: 0.36010941974818705\n",
      "Epoch: 16 Loss: 0.3141226787120104\n",
      "Epoch: 17 Loss: 0.27220613677054645\n",
      "Epoch: 18 Loss: 0.23939418867230416\n",
      "Epoch: 19 Loss: 0.20851999558508397\n",
      "Epoch: 20 Loss: 0.18667917959392072\n",
      "Epoch: 21 Loss: 0.15562305748462676\n",
      "Epoch: 22 Loss: 0.14083945490419864\n",
      "Epoch: 23 Loss: 0.12673422191292047\n",
      "Epoch: 24 Loss: 0.10901936031877994\n",
      "Epoch: 25 Loss: 0.0973500121384859\n",
      "Epoch: 26 Loss: 0.08817372210323811\n",
      "Epoch: 27 Loss: 0.07545622121542692\n",
      "Epoch: 28 Loss: 0.0648712420836091\n",
      "Epoch: 29 Loss: 0.06193406102247536\n",
      "Epoch: 30 Loss: 0.057034209556877616\n",
      "Epoch: 31 Loss: 0.05801574885845184\n",
      "Epoch: 32 Loss: 0.047175071015954015\n",
      "Epoch: 33 Loss: 0.04930704217404127\n",
      "Epoch: 34 Loss: 0.04800640698522329\n",
      "Epoch: 35 Loss: 0.042752913665026426\n",
      "Epoch: 36 Loss: 0.03901473961304873\n",
      "Epoch: 37 Loss: 0.03886506543494761\n",
      "Epoch: 38 Loss: 0.04044439978897572\n",
      "Epoch: 39 Loss: 0.02933169584721327\n",
      "Epoch: 40 Loss: 0.031108632852556183\n",
      "Epoch: 41 Loss: 0.024010288156569005\n",
      "Epoch: 42 Loss: 0.02874020943418145\n",
      "Epoch: 43 Loss: 0.03467275055591017\n",
      "Epoch: 44 Loss: 0.026674770331010223\n",
      "Epoch: 45 Loss: 0.03154696258716285\n",
      "Epoch: 46 Loss: 0.02197712949709967\n",
      "Epoch: 47 Loss: 0.025408950494602324\n",
      "Epoch: 48 Loss: 0.02548833596520126\n",
      "Epoch: 49 Loss: 0.03286439040675759\n",
      "Epoch: 50 Loss: 0.019393830420449377\n",
      "Epoch: 51 Loss: 0.01625969137530774\n",
      "Epoch: 52 Loss: 0.015466817491687834\n",
      "Epoch: 53 Loss: 0.016606400581076743\n",
      "Epoch: 54 Loss: 0.016617162246257068\n",
      "Epoch: 55 Loss: 0.01821252314839512\n",
      "Epoch: 56 Loss: 0.022897282388294117\n",
      "Epoch: 57 Loss: 0.013931691844481975\n",
      "Epoch: 58 Loss: 0.014107350178528577\n",
      "Epoch: 59 Loss: 0.014222973491996527\n",
      "Epoch: 60 Loss: 0.015392358088865877\n",
      "Epoch: 61 Loss: 0.013374446495436131\n",
      "Epoch: 62 Loss: 0.015115543897263706\n",
      "Epoch: 63 Loss: 0.0186222237884067\n",
      "Epoch: 64 Loss: 0.010900552908424288\n",
      "Epoch: 65 Loss: 0.01554260180273559\n",
      "Epoch: 66 Loss: 0.00872818159405142\n",
      "Epoch: 67 Loss: 0.010330604988848791\n",
      "Epoch: 68 Loss: 0.009044564759824425\n",
      "Epoch: 69 Loss: 0.00796658885665238\n",
      "Epoch: 70 Loss: 0.006850335677154362\n",
      "Epoch: 71 Loss: 0.008560027056955733\n",
      "Epoch: 72 Loss: 0.007998664997285233\n",
      "Epoch: 73 Loss: 0.0075107373500941325\n",
      "Epoch: 74 Loss: 0.014118985453387722\n",
      "Epoch: 75 Loss: 0.008539318302064202\n",
      "Epoch: 76 Loss: 0.006052647565957159\n",
      "Epoch: 77 Loss: 0.006637586146825925\n",
      "Epoch: 78 Loss: 0.006229125210666098\n",
      "Epoch: 79 Loss: 0.006810315151233226\n",
      "Epoch: 80 Loss: 0.009942643588874488\n",
      "Epoch: 81 Loss: 0.009588222834281623\n",
      "Epoch: 82 Loss: 0.006678845558781177\n",
      "Epoch: 83 Loss: 0.009053100881283171\n",
      "Epoch: 84 Loss: 0.006316717254230752\n",
      "Epoch: 85 Loss: 0.006420349999098107\n",
      "Epoch: 86 Loss: 0.0065146936220116915\n",
      "Epoch: 87 Loss: 0.004956811032025143\n",
      "Epoch: 88 Loss: 0.005244820858933963\n",
      "Epoch: 89 Loss: 0.0073105644405586645\n",
      "Epoch: 90 Loss: 0.008787299948744475\n",
      "Epoch: 91 Loss: 0.004937645856989548\n",
      "Epoch: 92 Loss: 0.003435403571347706\n",
      "Epoch: 93 Loss: 0.004694489676330705\n",
      "Epoch: 94 Loss: 0.003063014161307365\n",
      "Epoch: 95 Loss: 0.0031037892782478592\n",
      "Epoch: 96 Loss: 0.004924793860118371\n",
      "Epoch: 97 Loss: 0.004332086452632211\n",
      "Epoch: 98 Loss: 0.00827964064083062\n",
      "Epoch: 99 Loss: 0.014649888381245545\n",
      "Epoch: 100 Loss: 0.01213357486994937\n",
      "Epoch: 101 Loss: 0.005088224622886628\n",
      "Epoch: 102 Loss: 0.004516736735240556\n",
      "Epoch: 103 Loss: 0.005094558064593002\n",
      "Epoch: 104 Loss: 0.012781658201129175\n",
      "Epoch: 105 Loss: 0.014968892082106323\n",
      "Epoch: 106 Loss: 0.009105803113197908\n",
      "Epoch: 107 Loss: 0.00599141699494794\n",
      "Epoch: 108 Loss: 0.006290824717143551\n",
      "Epoch: 109 Loss: 0.007866280380403623\n",
      "Epoch: 110 Loss: 0.011198948870878666\n",
      "Epoch: 111 Loss: 0.006544165176455863\n",
      "Epoch: 112 Loss: 0.010280707859783434\n",
      "Epoch: 113 Loss: 0.005213760925107636\n",
      "Epoch: 114 Loss: 0.005374160094652325\n",
      "Epoch: 115 Loss: 0.004962243870249949\n",
      "Epoch: 116 Loss: 0.002941379795083776\n",
      "Epoch: 117 Loss: 0.002373635313415434\n",
      "Epoch: 118 Loss: 0.002092768631700892\n",
      "Epoch: 119 Loss: 0.002884841221384704\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint ./trained_model\n",
      "cats rule the world. dogs are the best. elephants have long trunks. monkeys like bananas. pandas eat bamboo. tigers are dangerous. zebras have stripes. lions are the kings of the savannah. giraffes have long necks. hippos are big and scary. rhinos have horns. penguins live in the arctic. polar bears are whithe e the sthest. oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n"
     ]
    }
   ],
   "source": [
    "class Runner(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def run(self):\n",
    "        # Create the tokenizer\n",
    "        tokenizer = Tokenizer()\n",
    "\n",
    "        embedding_dimension = 256\n",
    "        max_sequence_length = 20\n",
    "        number_of_tokens = tokenizer.size()\n",
    "\n",
    "        # Create the model\n",
    "        model = AutoregressiveWrapper(LanguageModel(\n",
    "            embedding_dimension=embedding_dimension,\n",
    "            number_of_tokens=number_of_tokens,\n",
    "            number_of_heads=4,\n",
    "            number_of_layers=3,\n",
    "            dropout_rate=0.1,\n",
    "            max_sequence_length=max_sequence_length\n",
    "        )).to(get_device())\n",
    "\n",
    "        # Create the training data\n",
    "        training_data = '. '.join([\n",
    "            'cats rule the world',\n",
    "            'dogs are the best',\n",
    "            'elephants have long trunks',\n",
    "            'monkeys like bananas',\n",
    "            'pandas eat bamboo',\n",
    "            'tigers are dangerous',\n",
    "            'zebras have stripes',\n",
    "            'lions are the kings of the savannah',\n",
    "            'giraffes have long necks',\n",
    "            'hippos are big and scary',\n",
    "            'rhinos have horns',\n",
    "            'penguins live in the arctic',\n",
    "            'polar bears are white'\n",
    "        ])\n",
    "\n",
    "        tokenized_and_padded_training_data = tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data)\n",
    "        sequences = create_training_sequences(max_sequence_length, tokenized_and_padded_training_data)\n",
    "\n",
    "        # Train the model\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        trainer = Trainer(model, tokenizer, optimizer)\n",
    "        loss_per_epoch = trainer.train(sequences, epochs=120, batch_size=16)\n",
    "\n",
    "        # Plot the loss per epoch in log scale\n",
    "        plt.plot(loss_per_epoch)\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.show()\n",
    "\n",
    "        model.save_checkpoint('./trained_model')\n",
    "\n",
    "        # Generate text\n",
    "        max_tokens_to_generate = 400\n",
    "        generator = Generator(model, tokenizer)\n",
    "        generated_text = generator.generate(\n",
    "            max_tokens_to_generate=max_tokens_to_generate,\n",
    "            prompt=\"cats\",\n",
    "            padding_token=tokenizer.character_to_token('<pad>')\n",
    "        )\n",
    "        print(generated_text.replace('<pad>', ''))\n",
    "\n",
    "\n",
    "def pad_left(sequence, final_length, padding_token):\n",
    "    return [padding_token] * (final_length - len(sequence)) + sequence\n",
    "\n",
    "\n",
    "Runner().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
